{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a14df5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T16:00:21.581819Z",
     "start_time": "2021-07-20T16:00:17.079273Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertForQuestionAnswering\n",
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145de56c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:42:25.918065Z",
     "start_time": "2021-07-17T07:42:22.723795Z"
    }
   },
   "outputs": [],
   "source": [
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f664f866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:42:29.594426Z",
     "start_time": "2021-07-17T07:42:25.969824Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97bc8d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:42:30.176013Z",
     "start_time": "2021-07-17T07:42:30.170529Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"How many parameters does BERT-large have?\"\n",
    "answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef44a2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:42:30.932959Z",
     "start_time": "2021-07-17T07:42:30.915846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 70 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b10e32fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:42:31.689680Z",
     "start_time": "2021-07-17T07:42:31.634952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "how           2,129\n",
      "many          2,116\n",
      "parameters   11,709\n",
      "does          2,515\n",
      "bert         14,324\n",
      "-             1,011\n",
      "large         2,312\n",
      "have          2,031\n",
      "?             1,029\n",
      "\n",
      "[SEP]           102\n",
      "\n",
      "bert         14,324\n",
      "-             1,011\n",
      "large         2,312\n",
      "is            2,003\n",
      "really        2,428\n",
      "big           2,502\n",
      ".             1,012\n",
      ".             1,012\n",
      ".             1,012\n",
      "it            2,009\n",
      "has           2,038\n",
      "24            2,484\n",
      "-             1,011\n",
      "layers        9,014\n",
      "and           1,998\n",
      "an            2,019\n",
      "em            7,861\n",
      "##bed         8,270\n",
      "##ding        4,667\n",
      "size          2,946\n",
      "of            1,997\n",
      "1             1,015\n",
      ",             1,010\n",
      "02            6,185\n",
      "##4           2,549\n",
      ",             1,010\n",
      "for           2,005\n",
      "a             1,037\n",
      "total         2,561\n",
      "of            1,997\n",
      "340          16,029\n",
      "##m           2,213\n",
      "parameters   11,709\n",
      "!               999\n",
      "altogether   10,462\n",
      "it            2,009\n",
      "is            2,003\n",
      "1             1,015\n",
      ".             1,012\n",
      "34            4,090\n",
      "##gb         18,259\n",
      ",             1,010\n",
      "so            2,061\n",
      "expect        5,987\n",
      "it            2,009\n",
      "to            2,000\n",
      "take          2,202\n",
      "a             1,037\n",
      "couple        3,232\n",
      "minutes       2,781\n",
      "to            2,000\n",
      "download      8,816\n",
      "to            2,000\n",
      "your          2,115\n",
      "cola         15,270\n",
      "##b           2,497\n",
      "instance      6,013\n",
      ".             1,012\n",
      "\n",
      "[SEP]           102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT only needs the token IDs, but for the purpose of inspecting the \n",
    "# tokenizer's behavior, let's also get the token strings and display them.\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "# For each token and its id...\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    \n",
    "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    \n",
    "    # Print the token string and its ID in two columns.\n",
    "    print('{:<12} {:>6,}'.format(token, id))\n",
    "\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d011a975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:42:32.863171Z",
     "start_time": "2021-07-17T07:42:32.594978Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, answer_text, return_tensors='pt')\n",
    "start_positions = torch.tensor([1])\n",
    "end_positions = torch.tensor([3])\n",
    "\n",
    "# outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb777d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:42:33.161930Z",
     "start_time": "2021-07-17T07:42:33.147710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"340 ##m\"\n"
     ]
    }
   ],
   "source": [
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores)\n",
    "\n",
    "# Combine the tokens in the answer and print it out.\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "385b9cda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:42:33.697382Z",
     "start_time": "2021-07-17T07:42:33.684993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"340m\"\n"
     ]
    }
   ],
   "source": [
    "# Start with the first token.\n",
    "answer = tokens[answer_start]\n",
    "\n",
    "# Select the remaining answer tokens and join them with whitespace.\n",
    "for i in range(answer_start + 1, answer_end + 1):\n",
    "    \n",
    "    # If it's a subword token, then recombine it with the previous token.\n",
    "    if tokens[i][0:2] == '##':\n",
    "        answer += tokens[i][2:]\n",
    "    \n",
    "    # Otherwise, add a space then the token.\n",
    "    else:\n",
    "        answer += ' ' + tokens[i]\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83ee2b8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:49:28.471938Z",
     "start_time": "2021-07-17T07:49:28.466114Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"How did you process picture\"\n",
    "answer_text = \"\"\"\n",
    "We took the dirt layer off, then we took the varnish layer off, and that allowed us to see the quality of the paint below: not only the colors, but the look of the paint. You can start seeing its age, the cracks, the abrasion pattern that you see in the early Netherlandish pictures, she explained.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca510177",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:49:32.438896Z",
     "start_time": "2021-07-17T07:49:32.427162Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(question, answer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4c2ed6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:49:41.804031Z",
     "start_time": "2021-07-17T07:49:41.795240Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55db52d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:49:42.506353Z",
     "start_time": "2021-07-17T07:49:42.330713Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, answer_text, return_tensors='pt')\n",
    "# outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0a26110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:49:42.865021Z",
     "start_time": "2021-07-17T07:49:42.851703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"var ##nish layer off , and that allowed us to see the quality of the paint below : not only the colors , but the look of the paint . you can start seeing its age , the cracks , the ab ##ras ##ion pattern that you see in the early net ##her ##land ##ish pictures\"\n"
     ]
    }
   ],
   "source": [
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores)\n",
    "\n",
    "# Combine the tokens in the answer and print it out.\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1201dc40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:48:38.708522Z",
     "start_time": "2021-07-17T07:48:38.691438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2129,  2106,  2017,  2832,  3861,   102,  2057,  2165,  1996,\n",
       "          6900,  6741,  2125,  1010,  2059,  2057,  2165,  1996, 13075, 24014,\n",
       "          6741,  2125,  1010,  1998,  2008,  3039,  2149,  2000,  2156,  1996,\n",
       "          3737,  1997,  1996,  6773,  2917,  1024,  2025,  2069,  1996,  6087,\n",
       "          1010,  2021,  1996,  2298,  1997,  1996,  6773,  1012,  2017,  2064,\n",
       "          2707,  3773,  2049,  2287,  1010,  1996, 15288,  1010,  1996, 11113,\n",
       "          8180,  3258,  5418,  2008,  2017,  2156,  1999,  1996,  2220,  5658,\n",
       "          5886,  3122,  4509,  4620,  1010,  2016,  4541,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7893d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
